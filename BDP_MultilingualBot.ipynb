{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDP Multilingual Bot\n",
    "\n",
    "---\n",
    "\n",
    "Based on multilingual bot model, i will try to train it in QA with English and Spanish QA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that i would use for train and test this case will be QA in English and Spanish I have combined for this model TriviaQA dataset and SQuAD_es dataset from this sites: \n",
    " - https://github.com/ccasimiro88/TranslateAlignRetrieve\n",
    " - http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I am going to create and use the information that i will use from TriviaQA dataset, selecting the params:\n",
    "- Question\n",
    "- Answer\n",
    "- Search Results that are the context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "url = \"http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\"\n",
    "target_path = 'triviaqa-unfiltered.tar.gz'\n",
    "\n",
    "#Download the file if it doesn't exist\n",
    "if target_path not in os.listdir():\n",
    "    print(\"Downloading file\")\n",
    "    # Dowload the file \n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Saving file\")\n",
    "        with open(target_path, 'wb') as f:\n",
    "            f.write(response.raw.read())\n",
    "\n",
    "    # Unzip the file\n",
    "    with tarfile.open(target_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welcome to GoComics.com, the world's largest c...</td>\n",
       "      <td>Who was President when the first Peanuts carto...</td>\n",
       "      <td>[Harry Truman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Nobel Prize in Literature 1930 Sinclair .....</td>\n",
       "      <td>Which American-born Sinclair won the Nobel Pri...</td>\n",
       "      <td>[Sinclair Lewis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dame Judi Dench is a renowned ... Born in Engl...</td>\n",
       "      <td>Where in England was Dame Judi Dench born?</td>\n",
       "      <td>[York]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our expert has answerd your question. Meet our...</td>\n",
       "      <td>William Christensen of Madison, New Jersey, ha...</td>\n",
       "      <td>[Beer Cans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Third Man Records Launches First Record Played...</td>\n",
       "      <td>In which decade did Billboard magazine first p...</td>\n",
       "      <td>[30s]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Welcome to GoComics.com, the world's largest c...   \n",
       "1  The Nobel Prize in Literature 1930 Sinclair .....   \n",
       "2  Dame Judi Dench is a renowned ... Born in Engl...   \n",
       "3  Our expert has answerd your question. Meet our...   \n",
       "4  Third Man Records Launches First Record Played...   \n",
       "\n",
       "                                            question           answers  \n",
       "0  Who was President when the first Peanuts carto...    [Harry Truman]  \n",
       "1  Which American-born Sinclair won the Nobel Pri...  [Sinclair Lewis]  \n",
       "2         Where in England was Dame Judi Dench born?            [York]  \n",
       "3  William Christensen of Madison, New Jersey, ha...       [Beer Cans]  \n",
       "4  In which decade did Billboard magazine first p...             [30s]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a  Train Dataframe from the json file with the variables that i need of TriviaQA\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "if 'triviaqa-unfiltered.csv' not in os.listdir():\n",
    "    # Load Json file\n",
    "    with open('./triviaqa-unfiltered/unfiltered-web-train.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    if isinstance(data, dict):\n",
    "        data_to_df = []\n",
    "        data_json = data[\"Data\"]\n",
    "        for item in data_json:\n",
    "            question = item.get('Question', '')  # Usar get para evitar errores si la clave no existe\n",
    "            answer = item.get('Answer', {}).get('Value', '')  # Anidando get para acceder a 'Value'\n",
    "            search_results = item.get('SearchResults', [])\n",
    "            context = \"\"\n",
    "            for searched in search_results:\n",
    "                context += searched.get('Description', '') + \" \"\n",
    "            data_to_df.append({\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'answers': [answer]\n",
    "            })\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        df_tr = pd.DataFrame(data_to_df)\n",
    "        # Save the DataFrame to a csv file\n",
    "        df_tr.to_csv('triviaqa-unfiltered.csv', index=False)\n",
    "        # Visualizar las primeras filas del DataFrame\n",
    "    else:\n",
    "        print(\"Json has not the dxpected format\")\n",
    "else:\n",
    "    df_tr = pd.read_csv('triviaqa-unfiltered.csv')\n",
    "    \n",
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice Cooper's The Man Behind the Mask Music V...</td>\n",
       "      <td>Who was the man behind The Chipmunks?</td>\n",
       "      <td>[David Seville]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jamie Lee Curtis, Actress: True Lies. Jamie Le...</td>\n",
       "      <td>What star sign is Jamie Lee Curtis?</td>\n",
       "      <td>[Scorpio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The official website for Andrew Lloyd Webber, ...</td>\n",
       "      <td>Which Lloyd Webber musical premiered in the US...</td>\n",
       "      <td>[Sunset Boulevard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The history and complete text of the 1917 Balf...</td>\n",
       "      <td>Who was the next British Prime Minister after ...</td>\n",
       "      <td>[Campbell-Bannerman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>... credits and award information for 70 Numbe...</td>\n",
       "      <td>Who had a 70s No 1 hit with Kiss You All Over?</td>\n",
       "      <td>[Exile]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Alice Cooper's The Man Behind the Mask Music V...   \n",
       "1  Jamie Lee Curtis, Actress: True Lies. Jamie Le...   \n",
       "2  The official website for Andrew Lloyd Webber, ...   \n",
       "3  The history and complete text of the 1917 Balf...   \n",
       "4  ... credits and award information for 70 Numbe...   \n",
       "\n",
       "                                            question               answers  \n",
       "0              Who was the man behind The Chipmunks?       [David Seville]  \n",
       "1                What star sign is Jamie Lee Curtis?             [Scorpio]  \n",
       "2  Which Lloyd Webber musical premiered in the US...    [Sunset Boulevard]  \n",
       "3  Who was the next British Prime Minister after ...  [Campbell-Bannerman]  \n",
       "4     Who had a 70s No 1 hit with Kiss You All Over?               [Exile]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a  Test Dataframe from the json file with the variables that i need of TriviaQA\n",
    "\n",
    "if 'test-triviaqa-unfiltered.csv' not in os.listdir():\n",
    "    # Load Json file\n",
    "    with open('./triviaqa-unfiltered/unfiltered-web-dev.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    if isinstance(data, dict):\n",
    "        data_to_df = []\n",
    "        data_json = data[\"Data\"]\n",
    "        for item in data_json:\n",
    "            question = item.get('Question', '')  # Usar get para evitar errores si la clave no existe\n",
    "            answer = item.get('Answer', {}).get('Value', '')  # Anidando get para acceder a 'Value'\n",
    "            search_results = item.get('SearchResults', [])\n",
    "            context = \"\"\n",
    "            for searched in search_results:\n",
    "                context += searched.get('Description', '') + \" \"\n",
    "            data_to_df.append({\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'answers': [answer]\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        test_tr = pd.DataFrame(data_to_df)\n",
    "        # Save the DataFrame to a csv file\n",
    "        test_tr.to_csv('test-triviaqa-unfiltered.csv', index=False)\n",
    "        # Visualizar las primeras filas del DataFrame\n",
    "\n",
    "    else:\n",
    "        print(\"Json has not the dxpected format\")\n",
    "else:\n",
    "    test_tr = pd.read_csv('test-triviaqa-unfiltered.csv')\n",
    "\n",
    "test_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i am going to prepare the SQuAD-es dataset, selecting:\n",
    "- Context\n",
    "- Questions\n",
    "- Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\BertBot\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "# Download dataset if it doesn't exist\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "target_path = 'squad_es'\n",
    "\n",
    "if target_path not in os.listdir():\n",
    "        # Dowload the file showing the progress bar\n",
    "    print(\"Downloading file\")\n",
    "    dataset = load_dataset(\"squad_es\", 'v1.1.0')  \n",
    "    dataset.save_to_disk(target_path)  \n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el conjunto de datos desde el disco\n",
    "if \"squad_train.csv\" not in os.listdir() or \"squad_test.csv\" not in os.listdir():\n",
    "    dataset = load_from_disk('squad_es')\n",
    "\n",
    "    # Convertir el conjunto de datos en DataFrame de Pandas\n",
    "    # Asumiendo que quieres convertir la parte 'train' del conjunto de datos\n",
    "    df_sq = pd.DataFrame(dataset['train'])\n",
    "    test_sq = pd.DataFrame(dataset['validation'])\n",
    "\n",
    "    # eliminate the columns that i dont need\n",
    "    df_sq.pop('id')\n",
    "    df_sq.pop('title')\n",
    "    test_sq.pop('id')\n",
    "    test_sq.pop('title')  \n",
    "    \n",
    "    df_sq['answers'] = df_sq['answers'].apply(lambda x: x['text'])\n",
    "    test_sq['answers'] = test_sq['answers'].apply(lambda x: x['text'])\n",
    "\n",
    "\n",
    "    df_sq.to_csv('squad_train.csv', index=False)\n",
    "    test_sq.to_csv('squad_test.csv', index=False)\n",
    "    \n",
    "\n",
    "else:\n",
    "    df_sq = pd.read_csv('squad_train.csv')\n",
    "    test_sq = pd.read_csv('squad_test.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, combine the two dataframes in one, mix the dataframes\n",
    "\n",
    "if 'train.csv' not in os.listdir() or 'test.csv' not in os.listdir():\n",
    "    df = pd.concat([df_tr, df_sq], ignore_index=True)\n",
    "    test = pd.concat([test_tr, test_sq], ignore_index=True)\n",
    "else:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have got both dataframes combined, lets tokenize the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) está disponible en tu sistema.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Verify if Cuda is available to use GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU) está disponible en tu sistema.\")\n",
    "else:\n",
    "    print(\"CUDA (GPU) no está disponible en tu sistema.\")\n",
    "\n",
    "# use multilingual cased model and its tokenizer\n",
    "tokenizer_cased = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model_cased = BertModel.from_pretrained('bert-base-multilingual-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(101), tensor(39728), tensor(10114), t...</td>\n",
       "      <td>[[tensor(101), tensor(14516), tensor(10134), t...</td>\n",
       "      <td>[[tensor(101), tensor(100), tensor(102), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(101), tensor(10117), tensor(16280), t...</td>\n",
       "      <td>[[tensor(101), tensor(160), tensor(39187), ten...</td>\n",
       "      <td>[[tensor(101), tensor(100), tensor(102), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(101), tensor(17803), tensor(45031), t...</td>\n",
       "      <td>[[tensor(101), tensor(23525), tensor(10106), t...</td>\n",
       "      <td>[[tensor(101), tensor(10482), tensor(102), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(101), tensor(18465), tensor(39957), t...</td>\n",
       "      <td>[[tensor(101), tensor(10694), tensor(52967), t...</td>\n",
       "      <td>[[tensor(101), tensor(100), tensor(102), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(101), tensor(16788), tensor(11343), t...</td>\n",
       "      <td>[[tensor(101), tensor(10167), tensor(10319), t...</td>\n",
       "      <td>[[tensor(101), tensor(100), tensor(102), tenso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  [[tensor(101), tensor(39728), tensor(10114), t...   \n",
       "1  [[tensor(101), tensor(10117), tensor(16280), t...   \n",
       "2  [[tensor(101), tensor(17803), tensor(45031), t...   \n",
       "3  [[tensor(101), tensor(18465), tensor(39957), t...   \n",
       "4  [[tensor(101), tensor(16788), tensor(11343), t...   \n",
       "\n",
       "                                            question  \\\n",
       "0  [[tensor(101), tensor(14516), tensor(10134), t...   \n",
       "1  [[tensor(101), tensor(160), tensor(39187), ten...   \n",
       "2  [[tensor(101), tensor(23525), tensor(10106), t...   \n",
       "3  [[tensor(101), tensor(10694), tensor(52967), t...   \n",
       "4  [[tensor(101), tensor(10167), tensor(10319), t...   \n",
       "\n",
       "                                             answers  \n",
       "0  [[tensor(101), tensor(100), tensor(102), tenso...  \n",
       "1  [[tensor(101), tensor(100), tensor(102), tenso...  \n",
       "2  [[tensor(101), tensor(10482), tensor(102), ten...  \n",
       "3  [[tensor(101), tensor(100), tensor(102), tenso...  \n",
       "4  [[tensor(101), tensor(100), tensor(102), tenso...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists('train.csv'):\n",
    "    # Function to tokenize the text\n",
    "\n",
    "    def tokenize(text):\n",
    "        return tokenizer_cased.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,#add [CLS], [SEP] (special tokens) at the beginning and end of the sentence\n",
    "            max_length=512, #maximum length of a sentence in tokens\n",
    "            padding='max_length', #add [PAD]s at the end of sentences to reach max_length\n",
    "            truncation=True, #truncates sentences to max_length if they exceed it\n",
    "            return_attention_mask=True, #return attention masks to know what tokens to attend to\n",
    "            return_tensors='pt' #return PyTorch tensors (also works with TensorFlow) being able to use them directly in PyTorch models with GPU\n",
    "        )\n",
    "\n",
    "    # Apply the tokenize function to the question and context columns\n",
    "    # input_ids are the tokenized text (number that represent the word in the vocabulary)\n",
    "    df['question'] = df['question'].apply(lambda x: tokenize(x)['input_ids']) #apply the tokenize function to the question column using the input_ids\n",
    "    test['question'] = test['question'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    df['context'] = df['context'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    test['context'] = test['context'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "\n",
    "    # due to i want a moder able to genereate a answer, i need to tokenize the answers\n",
    "    # in the other hand if i want to use a model to select an answer i dont need to tokenize the answers but i could do it anyways \n",
    "    df['answers'] = df['answers'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    test['answers'] = test['answers'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    \n",
    "    df.to_csv('train.csv', index=False)\n",
    "    test.to_csv('test.csv', index=False)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenize and convert them into tensors, it is time to create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train, validation and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "# Create Data Tensor\n",
    "train_dataset = TensorDataset(train_df['input_ids'], train_df['attention_mask'])\n",
    "val_dataset = TensorDataset(val_df['input_ids'], val_df['attention_mask'])\n",
    "test_dataset = TensorDataset(test['input_ids'], test['attention_mask'])\n",
    "\n",
    "# Create Data Loader\n",
    "batch_size = 16  # Ajusta esto según la capacidad de tu GPU\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
