{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDP Multilingual Bot\n",
    "\n",
    "---\n",
    "\n",
    "Based on multilingual bot model, i will try to train it in QA with English and Spanish QA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that i would use for train and test this case will be QA in English and Spanish I have combined for this model TriviaQA dataset and SQuAD_es dataset from this sites: \n",
    " - https://github.com/ccasimiro88/TranslateAlignRetrieve\n",
    " - http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\n",
    "\n",
    " This model, as part of BERT will be espcialist in answer questiosn, but needing a context, or a multiple choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I am going to create and use the information that i will use from TriviaQA dataset, selecting the params:\n",
    "- Question\n",
    "- Answer\n",
    "- Search Results that are the context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "url = \"http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\"\n",
    "target_path = 'triviaqa-unfiltered.tar.gz'\n",
    "\n",
    "#Download the file if it doesn't exist\n",
    "if target_path not in os.listdir():\n",
    "    print(\"Downloading file\")\n",
    "    # Dowload the file \n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Saving file\")\n",
    "        with open(target_path, 'wb') as f:\n",
    "            f.write(response.raw.read())\n",
    "\n",
    "    # Unzip the file\n",
    "    with tarfile.open(target_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJson has not the dxpected format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     df_tr \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtriviaqa-unfiltered.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m df_tr\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m     36\u001b[0m df_tr\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\BertBot\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\BertBot\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\BertBot\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32me:\\BertBot\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# create a  Train Dataframe from the json file with the variables that i need of TriviaQA\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "if 'triviaqa-unfiltered.csv' not in os.listdir():\n",
    "    # Load Json file\n",
    "    with open('./triviaqa-unfiltered/unfiltered-web-train.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    if isinstance(data, dict):\n",
    "        data_to_df = []\n",
    "        data_json = data[\"Data\"]\n",
    "        for item in data_json:\n",
    "            question = item.get('Question', '')  # Usar get para evitar errores si la clave no existe\n",
    "            answer = item.get('Answer', {}).get('Value', '')  # Anidando get para acceder a 'Value'\n",
    "            search_results = item.get('SearchResults', [])\n",
    "            context = \"Example context:\"\n",
    "            for searched in search_results:\n",
    "                context += searched.get('Description', '') + \" \"\n",
    "            data_to_df.append({\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'answers': [answer]\n",
    "            })\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        df_tr = pd.DataFrame(data_to_df)\n",
    "        # Save the DataFrame to a csv file\n",
    "        df_tr.to_csv('triviaqa-unfiltered.csv', index=False)\n",
    "        # Visualizar las primeras filas del DataFrame\n",
    "    else:\n",
    "        print(\"Json has not the dxpected format\")\n",
    "else:\n",
    "    df_tr = pd.read_csv('triviaqa-unfiltered.csv')\n",
    "    \n",
    "df_tr.head()\n",
    "df_tr.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a  Test Dataframe from the json file with the variables that i need of TriviaQA\n",
    "\n",
    "if 'test-triviaqa-unfiltered.csv' not in os.listdir():\n",
    "    # Load Json file\n",
    "    with open('./triviaqa-unfiltered/unfiltered-web-dev.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    if isinstance(data, dict):\n",
    "        data_to_df = []\n",
    "        data_json = data[\"Data\"]\n",
    "        for item in data_json:\n",
    "            question = item.get('Question', '')  # Usar get para evitar errores si la clave no existe\n",
    "            answer = item.get('Answer', {}).get('Value', '')  # Anidando get para acceder a 'Value'\n",
    "            search_results = item.get('SearchResults', [])\n",
    "            context = \"\"\n",
    "            for searched in search_results:\n",
    "                context += searched.get('Description', '') + \" \"\n",
    "            data_to_df.append({\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'answers': [answer]\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        test_tr = pd.DataFrame(data_to_df)\n",
    "        # Save the DataFrame to a csv file\n",
    "        test_tr.to_csv('test-triviaqa-unfiltered.csv', index=False)\n",
    "        # Visualizar las primeras filas del DataFrame\n",
    "\n",
    "    else:\n",
    "        print(\"Json has not the dxpected format\")\n",
    "else:\n",
    "    test_tr = pd.read_csv('test-triviaqa-unfiltered.csv')\n",
    "test_tr.dropna(inplace=True)\n",
    "test_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i am going to prepare the SQuAD-es dataset, selecting:\n",
    "- Context\n",
    "- Questions\n",
    "- Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset if it doesn't exist\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "target_path = 'squad_es'\n",
    "\n",
    "if target_path not in os.listdir():\n",
    "        # Dowload the file showing the progress bar\n",
    "    print(\"Downloading file\")\n",
    "    dataset = load_dataset(\"squad_es\", 'v1.1.0')  \n",
    "    dataset.save_to_disk(target_path)  \n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el conjunto de datos desde el disco\n",
    "if \"squad_train.csv\" not in os.listdir() or \"squad_test.csv\" not in os.listdir():\n",
    "    dataset = load_from_disk('squad_es')\n",
    "\n",
    "    # Convertir el conjunto de datos en DataFrame de Pandas\n",
    "    # Asumiendo que quieres convertir la parte 'train' del conjunto de datos\n",
    "    df_sq = pd.DataFrame(dataset['train'])\n",
    "    test_sq = pd.DataFrame(dataset['validation'])\n",
    "\n",
    "    # eliminate the columns that i dont need\n",
    "    df_sq.pop('id')\n",
    "    df_sq.pop('title')\n",
    "    test_sq.pop('id')\n",
    "    test_sq.pop('title')  \n",
    "    \n",
    "    df_sq['answers'] = df_sq['answers'].apply(lambda x: x['text'])\n",
    "    test_sq['answers'] = test_sq['answers'].apply(lambda x: x['text'])\n",
    "\n",
    "\n",
    "    df_sq.to_csv('squad_train.csv', index=False)\n",
    "    test_sq.to_csv('squad_test.csv', index=False)\n",
    "    \n",
    "\n",
    "else:\n",
    "    df_sq = pd.read_csv('squad_train.csv')\n",
    "    test_sq = pd.read_csv('squad_test.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, combine the two dataframes in one, mix the dataframes\n",
    "\n",
    "if 'train.csv' not in os.listdir() or 'test.csv' not in os.listdir():\n",
    "    df = pd.concat([df_tr, df_sq], ignore_index=True)\n",
    "    test = pd.concat([test_tr, test_sq], ignore_index=True)\n",
    "else:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have got both dataframes combined, lets tokenize the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Verify if Cuda is available to use GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU) está disponible en tu sistema.\")\n",
    "else:\n",
    "    print(\"CUDA (GPU) no está disponible en tu sistema.\")\n",
    "\n",
    "# use multilingual cased model and its tokenizer\n",
    "tokenizer_cased = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# fucntion to get the start and end indexes \n",
    "def find_start_end_indices(row):\n",
    "    # Tokenize the context\n",
    "    context_tokens = tokenizer_cased.encode(row['context'], add_special_tokens=False)\n",
    "    answer_tokens = tokenizer_cased.encode(row['answer'], add_special_tokens=False)\n",
    "\n",
    "    # search start an end context\n",
    "    start_index = None\n",
    "    end_index = None\n",
    "    for i in range(len(context_tokens)):\n",
    "        if context_tokens[i:i+len(answer_tokens)] == answer_tokens:\n",
    "            start_index = i\n",
    "            end_index = i + len(answer_tokens) - 1\n",
    "            break\n",
    "\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists('train.csv'):\n",
    "    # Function to tokenize the text\n",
    "    print(\"Tokenizing the text\")\n",
    "    def tokenize(text):\n",
    "        return tokenizer_cased.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,#add [CLS], [SEP] (special tokens) at the beginning and end of the sentence\n",
    "            max_length=512, #maximum length of a sentence in tokens\n",
    "            padding='max_length', #add [PAD]s at the end of sentences to reach max_length\n",
    "            truncation=True, #truncates sentences to max_length if they exceed it\n",
    "            return_attention_mask=True, #return attention masks to know what tokens to attend to\n",
    "            return_tensors='pt' #return PyTorch tensors (also works with TensorFlow) being able to use them directly in PyTorch models with GPU\n",
    "        )\n",
    "\n",
    "    # Apply the tokenize function to the question and context columns\n",
    "    # input_ids are the tokenized text (number that represent the word in the vocabulary)\n",
    "    df['input_ids'] = df['question'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    df['attention_mask'] = df['question'].apply(lambda x: tokenize(x)['attention_mask'])\n",
    "    \n",
    "    # obtain the start and en indexes in order to give the clue tht must search in the context\n",
    "    df[['start_index', 'end_index']] = df.apply(lambda row: find_start_end_indices(row), axis=1, result_type='expand')\n",
    "    test[['start_index', 'end_index']] = df.apply(lambda row: find_start_end_indices(row), axis=1, result_type='expand')\n",
    "\n",
    "    test['input_ids'] = test['question'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    test['attention_mask'] = test['question'].apply(lambda x: tokenize(x)['attention_mask'])\n",
    "    df['question'] = df['question'].apply(lambda x: tokenize(x)['input_ids']) #apply the tokenize function to the question column using the input_ids\n",
    "    test['question'] = test['question'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    df['context'] = df['context'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    test['context'] = test['context'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "\n",
    "    # due to i want a moder able to genereate a answer, i need to tokenize the answers\n",
    "    # in the other hand if i want to use a model to select an answer i dont need to tokenize the answers but i could do it anyways \n",
    "    df['answers'] = df['answers'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    test['answers'] = test['answers'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "\n",
    "\n",
    "    df.to_pickle('train.pkl')\n",
    "    test.to_pickle('test.pkl')\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Loading the tokenized text\")\n",
    "    df = pd.read_pickle('train.pkl')\n",
    "    test = pd.read_pickle('test.pkl')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenize and convert them into tensors, it is time to create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train, validation and test sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "import torch\n",
    "# Suponiendo que df es tu DataFrame y que las columnas 'input_ids' y 'attention_mask' contienen tensores de PyTorch\n",
    "train_dataset = TensorDataset(torch.stack(train_df['input_ids'].tolist()), torch.stack(train_df['attention_mask'].tolist()))\n",
    "test_dataset = TensorDataset(torch.stack(test['input_ids'].tolist()), torch.stack(test['attention_mask'].tolist()))\n",
    "val_dataset = TensorDataset(torch.stack(val_df['input_ids'].tolist()), torch.stack(val_df['attention_mask'].tolist()))\n",
    "\n",
    "# Create Data Loader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertForQuestionAnswering, BertForMultipleChoice, AdamW\n",
    "# load this 3 models in order to compare the results of them for this case of use.\n",
    "\n",
    "model_cased = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')\n",
    "mulchoice_model = BertForMultipleChoice.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generic function to pass the model that i want to train and test, also load the model from a checkpoint saved if exists \n",
    "\n",
    "def train_test_model(model, name):\n",
    "    # use cuda if available in order to train it faster\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # uses Adam, in the future change to tests other models\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
