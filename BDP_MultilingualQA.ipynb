{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDP Multilingual Bot for QA\n",
    "\n",
    "---\n",
    "\n",
    "Based on multilingual bot model, i will try to train it in QA with English and Spanish QA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that i would use for train and test this case will be QA in English and Spanish I have combined for this model TriviaQA dataset and SQuAD_es dataset from this sites: \n",
    " - https://github.com/ccasimiro88/TranslateAlignRetrieve\n",
    " - http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\n",
    "\n",
    " This model, as part of BERT will be espcialist in answer questiosn, but needing a context, or a multiple choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I am going to create and use the information that i will use from TriviaQA dataset, selecting the params:\n",
    "- Question\n",
    "- Answer\n",
    "- Search Results that are the context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "url = \"http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\"\n",
    "target_path = 'triviaqa-unfiltered.tar.gz'\n",
    "\n",
    "#Download the file if it doesn't exist\n",
    "if target_path not in os.listdir():\n",
    "    print(\"Downloading file\")\n",
    "    # Dowload the file \n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Saving file\")\n",
    "        with open(target_path, 'wb') as f:\n",
    "            f.write(response.raw.read())\n",
    "\n",
    "    # Unzip the file\n",
    "    with tarfile.open(target_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a  Train Dataframe from the json file with the variables that i need of TriviaQA\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "if 'triviaqa-unfiltered.csv' not in os.listdir():\n",
    "    # Load Json file\n",
    "    with open('./triviaqa-unfiltered/unfiltered-web-train.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    if isinstance(data, dict):\n",
    "        data_to_df = []\n",
    "        data_json = data[\"Data\"]\n",
    "        for item in data_json:\n",
    "            question = item.get('Question', '')  # Usar get para evitar errores si la clave no existe\n",
    "            answer = item.get('Answer', {}).get('Value', '')  # Anidando get para acceder a 'Value'\n",
    "            search_results = item.get('SearchResults', [])\n",
    "            context = \"Example context:\"\n",
    "            for searched in search_results:\n",
    "                context += searched.get('Description', '') + \" \"\n",
    "            data_to_df.append({\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'answers': [answer]\n",
    "            })\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        df_tr = pd.DataFrame(data_to_df)\n",
    "        # Save the DataFrame to a csv file\n",
    "        df_tr.to_csv('triviaqa-unfiltered.csv', index=False)\n",
    "        # Visualizar las primeras filas del DataFrame\n",
    "    else:\n",
    "        print(\"Json has not the dxpected format\")\n",
    "else:\n",
    "    df_tr = pd.read_csv('triviaqa-unfiltered.csv')\n",
    "    \n",
    "df_tr.head()\n",
    "df_tr.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice Cooper's The Man Behind the Mask Music V...</td>\n",
       "      <td>Who was the man behind The Chipmunks?</td>\n",
       "      <td>['David Seville']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jamie Lee Curtis, Actress: True Lies. Jamie Le...</td>\n",
       "      <td>What star sign is Jamie Lee Curtis?</td>\n",
       "      <td>['Scorpio']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The official website for Andrew Lloyd Webber, ...</td>\n",
       "      <td>Which Lloyd Webber musical premiered in the US...</td>\n",
       "      <td>['Sunset Boulevard']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The history and complete text of the 1917 Balf...</td>\n",
       "      <td>Who was the next British Prime Minister after ...</td>\n",
       "      <td>['Campbell-Bannerman']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>... credits and award information for 70 Numbe...</td>\n",
       "      <td>Who had a 70s No 1 hit with Kiss You All Over?</td>\n",
       "      <td>['Exile']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Alice Cooper's The Man Behind the Mask Music V...   \n",
       "1  Jamie Lee Curtis, Actress: True Lies. Jamie Le...   \n",
       "2  The official website for Andrew Lloyd Webber, ...   \n",
       "3  The history and complete text of the 1917 Balf...   \n",
       "4  ... credits and award information for 70 Numbe...   \n",
       "\n",
       "                                            question                 answers  \n",
       "0              Who was the man behind The Chipmunks?       ['David Seville']  \n",
       "1                What star sign is Jamie Lee Curtis?             ['Scorpio']  \n",
       "2  Which Lloyd Webber musical premiered in the US...    ['Sunset Boulevard']  \n",
       "3  Who was the next British Prime Minister after ...  ['Campbell-Bannerman']  \n",
       "4     Who had a 70s No 1 hit with Kiss You All Over?               ['Exile']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a  Test Dataframe from the json file with the variables that i need of TriviaQA\n",
    "\n",
    "if 'test-triviaqa-unfiltered.csv' not in os.listdir():\n",
    "    # Load Json file\n",
    "    with open('./triviaqa-unfiltered/unfiltered-web-dev.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    if isinstance(data, dict):\n",
    "        data_to_df = []\n",
    "        data_json = data[\"Data\"]\n",
    "        for item in data_json:\n",
    "            question = item.get('Question', '')  # Usar get para evitar errores si la clave no existe\n",
    "            answer = item.get('Answer', {}).get('Value', '')  # Anidando get para acceder a 'Value'\n",
    "            search_results = item.get('SearchResults', [])\n",
    "            context = \"\"\n",
    "            for searched in search_results:\n",
    "                context += searched.get('Description', '') + \" \"\n",
    "            data_to_df.append({\n",
    "                'context': context,\n",
    "                'question': question,\n",
    "                'answers': [answer]\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        test_tr = pd.DataFrame(data_to_df)\n",
    "        # Save the DataFrame to a csv file\n",
    "        test_tr.to_csv('test-triviaqa-unfiltered.csv', index=False)\n",
    "        # Visualizar las primeras filas del DataFrame\n",
    "\n",
    "    else:\n",
    "        print(\"Json has not the dxpected format\")\n",
    "else:\n",
    "    test_tr = pd.read_csv('test-triviaqa-unfiltered.csv')\n",
    "test_tr.dropna(inplace=True)\n",
    "test_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i am going to prepare the SQuAD-es dataset, selecting:\n",
    "- Context\n",
    "- Questions\n",
    "- Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\BertBot\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "# Download dataset if it doesn't exist\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "target_path = 'squad_es'\n",
    "\n",
    "if target_path not in os.listdir():\n",
    "        # Dowload the file showing the progress bar\n",
    "    print(\"Downloading file\")\n",
    "    dataset = load_dataset(\"squad_es\", 'v1.1.0')  \n",
    "    dataset.save_to_disk(target_path)  \n",
    "else:\n",
    "    print(\"File already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el conjunto de datos desde el disco\n",
    "if \"squad_train.csv\" not in os.listdir() or \"squad_test.csv\" not in os.listdir():\n",
    "    dataset = load_from_disk('squad_es')\n",
    "\n",
    "    # Convertir el conjunto de datos en DataFrame de Pandas\n",
    "    # Asumiendo que quieres convertir la parte 'train' del conjunto de datos\n",
    "    df_sq = pd.DataFrame(dataset['train'])\n",
    "    test_sq = pd.DataFrame(dataset['validation'])\n",
    "\n",
    "    # eliminate the columns that i dont need\n",
    "    df_sq.pop('id')\n",
    "    df_sq.pop('title')\n",
    "    test_sq.pop('id')\n",
    "    test_sq.pop('title')  \n",
    "    \n",
    "    df_sq['answers'] = df_sq['answers'].apply(lambda x: x['text'])\n",
    "    test_sq['answers'] = test_sq['answers'].apply(lambda x: x['text'])\n",
    "\n",
    "\n",
    "    df_sq.to_csv('squad_train.csv', index=False)\n",
    "    test_sq.to_csv('squad_test.csv', index=False)\n",
    "    \n",
    "\n",
    "else:\n",
    "    df_sq = pd.read_csv('squad_train.csv')\n",
    "    test_sq = pd.read_csv('squad_test.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, combine the two dataframes in one, mix the dataframes\n",
    "\n",
    "if 'train.csv' not in os.listdir() or 'test.csv' not in os.listdir():\n",
    "    df = pd.concat([df_tr, df_sq], ignore_index=True)\n",
    "    test = pd.concat([test_tr, test_sq], ignore_index=True)\n",
    "else:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have got both dataframes combined, lets tokenize the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) está disponible en tu sistema.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Verify if Cuda is available to use GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU) está disponible en tu sistema.\")\n",
    "else:\n",
    "    print(\"CUDA (GPU) no está disponible en tu sistema.\")\n",
    "\n",
    "# use multilingual cased model and its tokenizer\n",
    "tokenizer_cased = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# fucntion to get the start and end indexes \n",
    "def find_start_end_indices(row):\n",
    "    # Tokenize the context\n",
    "    context_tokens = tokenizer_cased.encode(row['context'], add_special_tokens=False)\n",
    "    answer_tokens = tokenizer_cased.encode(row['answer'], add_special_tokens=False)\n",
    "\n",
    "    # search start an end context\n",
    "    start_index = None\n",
    "    end_index = None\n",
    "    for i in range(len(context_tokens)):\n",
    "        if context_tokens[i:i+len(answer_tokens)] == answer_tokens:\n",
    "            start_index = i\n",
    "            end_index = i + len(answer_tokens) - 1\n",
    "            break\n",
    "\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the tokenized text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(101), tensor(27746), tensor(53692), t...</td>\n",
       "      <td>[[tensor(101), tensor(14516), tensor(10134), t...</td>\n",
       "      <td>[[tensor(101), tensor(164), tensor(112), tenso...</td>\n",
       "      <td>[[tensor(101), tensor(14516), tensor(10134), t...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(101), tensor(27746), tensor(53692), t...</td>\n",
       "      <td>[[tensor(101), tensor(160), tensor(39187), ten...</td>\n",
       "      <td>[[tensor(101), tensor(164), tensor(112), tenso...</td>\n",
       "      <td>[[tensor(101), tensor(160), tensor(39187), ten...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(101), tensor(27746), tensor(53692), t...</td>\n",
       "      <td>[[tensor(101), tensor(23525), tensor(10106), t...</td>\n",
       "      <td>[[tensor(101), tensor(164), tensor(112), tenso...</td>\n",
       "      <td>[[tensor(101), tensor(23525), tensor(10106), t...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(101), tensor(27746), tensor(53692), t...</td>\n",
       "      <td>[[tensor(101), tensor(10694), tensor(52967), t...</td>\n",
       "      <td>[[tensor(101), tensor(164), tensor(112), tenso...</td>\n",
       "      <td>[[tensor(101), tensor(10694), tensor(52967), t...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(101), tensor(27746), tensor(53692), t...</td>\n",
       "      <td>[[tensor(101), tensor(10167), tensor(10319), t...</td>\n",
       "      <td>[[tensor(101), tensor(164), tensor(112), tenso...</td>\n",
       "      <td>[[tensor(101), tensor(10167), tensor(10319), t...</td>\n",
       "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  [[tensor(101), tensor(27746), tensor(53692), t...   \n",
       "1  [[tensor(101), tensor(27746), tensor(53692), t...   \n",
       "2  [[tensor(101), tensor(27746), tensor(53692), t...   \n",
       "3  [[tensor(101), tensor(27746), tensor(53692), t...   \n",
       "4  [[tensor(101), tensor(27746), tensor(53692), t...   \n",
       "\n",
       "                                            question  \\\n",
       "0  [[tensor(101), tensor(14516), tensor(10134), t...   \n",
       "1  [[tensor(101), tensor(160), tensor(39187), ten...   \n",
       "2  [[tensor(101), tensor(23525), tensor(10106), t...   \n",
       "3  [[tensor(101), tensor(10694), tensor(52967), t...   \n",
       "4  [[tensor(101), tensor(10167), tensor(10319), t...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  [[tensor(101), tensor(164), tensor(112), tenso...   \n",
       "1  [[tensor(101), tensor(164), tensor(112), tenso...   \n",
       "2  [[tensor(101), tensor(164), tensor(112), tenso...   \n",
       "3  [[tensor(101), tensor(164), tensor(112), tenso...   \n",
       "4  [[tensor(101), tensor(164), tensor(112), tenso...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [[tensor(101), tensor(14516), tensor(10134), t...   \n",
       "1  [[tensor(101), tensor(160), tensor(39187), ten...   \n",
       "2  [[tensor(101), tensor(23525), tensor(10106), t...   \n",
       "3  [[tensor(101), tensor(10694), tensor(52967), t...   \n",
       "4  [[tensor(101), tensor(10167), tensor(10319), t...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "1  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "2  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "3  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  \n",
       "4  [[tensor(1), tensor(1), tensor(1), tensor(1), ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists('train.pkl'):\n",
    "    # Function to tokenize the text\n",
    "    print(\"Tokenizing the text\")\n",
    "    def tokenize(text):\n",
    "        return tokenizer_cased.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,#add [CLS], [SEP] (special tokens) at the beginning and end of the sentence\n",
    "            max_length=512, #maximum length of a sentence in tokens\n",
    "            padding='max_length', #add [PAD]s at the end of sentences to reach max_length\n",
    "            truncation=True, #truncates sentences to max_length if they exceed it\n",
    "            return_attention_mask=True, #return attention masks to know what tokens to attend to\n",
    "            return_tensors='pt' #return PyTorch tensors (also works with TensorFlow) being able to use them directly in PyTorch models with GPU\n",
    "        )\n",
    "\n",
    "    # Apply the tokenize function to the question and context columns\n",
    "    # input_ids are the tokenized text (number that represent the word in the vocabulary)\n",
    "    df['input_ids'] = df['question'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    df['attention_mask'] = df['question'].apply(lambda x: tokenize(x)['attention_mask'])\n",
    "    \n",
    "    # obtain the start and en indexes in order to give the clue tht must search in the context\n",
    "    # df[['start_index', 'end_index']] = df.apply(lambda row: find_start_end_indices(row), axis=1, result_type='expand')\n",
    "    # test[['start_index', 'end_index']] = df.apply(lambda row: find_start_end_indices(row), axis=1, result_type='expand')\n",
    "\n",
    "    test['input_ids'] = test['question'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    test['attention_mask'] = test['question'].apply(lambda x: tokenize(x)['attention_mask'])\n",
    "    df['question'] = df['question'].apply(lambda x: tokenize(x)['input_ids']) #apply the tokenize function to the question column using the input_ids\n",
    "    test['question'] = test['question'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    df['context'] = df['context'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    test['context'] = test['context'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "\n",
    "    # due to i want a moder able to genereate a answer, i need to tokenize the answers\n",
    "    # in the other hand if i want to use a model to select an answer i dont need to tokenize the answers but i could do it anyways \n",
    "    df['answers'] = df['answers'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "    test['answers'] = test['answers'].apply(lambda x: tokenize(x)['input_ids'])\n",
    "\n",
    "\n",
    "    df.to_pickle('train.pkl')\n",
    "    test.to_pickle('test.pkl')\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Loading the tokenized text\")\n",
    "    df = pd.read_pickle('train.pkl')\n",
    "    test = pd.read_pickle('test.pkl')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenize and convert them into tensors, it is time to create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train, validation and test sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "import torch\n",
    "# Suponiendo que df es tu DataFrame y que las columnas 'input_ids' y 'attention_mask' contienen tensores de PyTorch\n",
    "train_dataset = TensorDataset(torch.stack(train_df['input_ids'].tolist()), torch.stack(train_df['attention_mask'].tolist()))\n",
    "test_dataset = TensorDataset(torch.stack(test['input_ids'].tolist()), torch.stack(test['attention_mask'].tolist()))\n",
    "val_dataset = TensorDataset(torch.stack(val_df['input_ids'].tolist()), torch.stack(val_df['attention_mask'].tolist()))\n",
    "\n",
    "# Create Data Loader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertForQuestionAnswering, BertForMultipleChoice, AdamW\n",
    "# load this 3 models in order to compare the results of them for this case of use.\n",
    "\n",
    "model_cased = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')\n",
    "mulchoice_model = BertForMultipleChoice.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generic function to pass the model that i want to train and test, also load the model from a checkpoint saved if exists \n",
    "\n",
    "def train_test_model(model, name):\n",
    "    # use cuda if available in order to train it faster\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # uses Adam, in the future change to tests other models\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
